{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXTZ3sthLr87"
   },
   "source": [
    "# **FolT Homework 10**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eo3mepfqL17n"
   },
   "source": [
    "### Homework 10.1 (10 points)\n",
    "For the sake of science, let’s assume that we are on the other side as a spammer.\n",
    "\n",
    "(a) What could you do to avoid your spam being detected with respect to the previously implemented features?\n",
    "\n",
    "- Formulate at least two counter-strategies. (2 points)\n",
    "\n",
    "Keep in mind that as a spammer all you need is the short attention of\n",
    "the reader for the main message. It does not really matter what the rest of the email looks like. Also the spam\n",
    "message only needs to be readable, not unaltered. Please take care, that you only change the spam messages\n",
    "(the messages from the corpus in the training and the test data that belong to the SPAM class), not the valid\n",
    "emails, as (hopefully) the spammer has no way of changing the valid emails on the user’s computer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_tNVwBc_0rP"
   },
   "source": [
    "__Answer:__\n",
    "\n",
    "- Removes features that is famous for spam such as repeated characters, in general we want to clean the text\n",
    "\n",
    "- Add good word that are often associate with good email "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tGGj8oPzAA9"
   },
   "source": [
    "(b) Implement your ideas by adding a method that changes the content of spam emails before they are feed into\n",
    "the classifier for training.\n",
    "\n",
    "- write a function to read the mails from the ```emails.zip```. You may use the ```LazyCorpusLoader``` (1 point)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nospam', 'spam']\n",
      "481\n",
      "2412\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "from nltk.corpus.reader import CategorizedPlaintextCorpusReader \n",
    "\n",
    "mails_true = LazyCorpusLoader('mails', CategorizedPlaintextCorpusReader,r'(?!\\.).*\\.txt', cat_pattern=r'(spam|nospam)/.*')\n",
    "\n",
    "print(mails_true.categories()) #['nospam', 'spam']\n",
    "print(len(mails_true.fileids('spam'))) #481\n",
    "print(len(mails_true.fileids('nospam'))) #2412"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- implement at least two of your ideas from (a) (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#1. First approach: remove special character and repeated character\n",
    "def clean_txt(text):\n",
    "    text = re.sub(r\"[^A-Za-z ]+\", ' ',text) \n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "    text = re.sub(r\"[^A-Za-z ]+\", '',text) \n",
    "    text = [tok for tok in text.split() if len(tok)>1 ]\n",
    "    text = \" \".join(text)\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name of all spam *.txt file\n",
    "\n",
    "spam_txt =[]\n",
    "for root, dirs, files in os.walk(\"emails\\spam\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "             spam_txt.append(os.path.join(root, file))\n",
    "\n",
    "for path in spam_txt:\n",
    "    with open(path,\"r\") as f: \n",
    "        text = f.read()\n",
    "        text_modify = clean_txt(text)\n",
    "    \n",
    "    filename = \"emails/spam1/\"+ path.split(\"\\\\\")[-1]\n",
    "    with open(filename,\"w\") as f:\n",
    "        f.write(text_modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "from random import sample\n",
    "\n",
    "# 2. Approach add a number of random words in the vocabulary \n",
    "def add_valid_word(text, num = 300):\n",
    "    rand_words = ' '.join(sample(words.words(), num))\n",
    "    text = text + rand_words\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Get the name of all spam *.txt file\n",
    "\n",
    "spam_txt =[]\n",
    "for root, dirs, files in os.walk(\"emails\\spam\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "             spam_txt.append(os.path.join(root, file))\n",
    "\n",
    "for path in spam_txt:\n",
    "    with open(path,\"r\") as f: \n",
    "        text = f.read()\n",
    "        text = clean_txt(text)\n",
    "        text_modify = add_valid_word(text)\n",
    "    \n",
    "    filename = \"emails/spam2/\"+ path.split(\"\\\\\")[-1]\n",
    "    with open(filename,\"w\") as f:\n",
    "        f.write(text_modify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkV6jupU_QgV"
   },
   "source": [
    "- Use the classifier that you implemented in the Exercise 10. (1 point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_PRF(gold, predicted, class_label):\n",
    "    if len(gold) != len(predicted):\n",
    "        raise ValueError('Sizes of gold standard and predicted value need to be equal.')\n",
    "\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(len(gold)):\n",
    "        if gold[i] == class_label and predicted[i] == class_label:\n",
    "            TP += 1\n",
    "        elif not gold[i] == class_label and predicted[i] == class_label:\n",
    "            FP += 1\n",
    "        elif gold[i] == class_label and not predicted[i] == class_label:\n",
    "            FN += 1\n",
    "\n",
    "    if (TP+FP > 0):\n",
    "        precision = TP/(TP+FP)\n",
    "    else:\n",
    "        precision = 0\n",
    "\n",
    "    if (TP+FN > 0):\n",
    "        recall = TP/(TP+FN)\n",
    "    else:\n",
    "        recall = 0\n",
    "\n",
    "    if precision > 0 and recall > 0:\n",
    "        fmeasure = 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        fmeasure = 0\n",
    "\n",
    "    return (precision, recall, fmeasure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, random, logging, math\n",
    "from corpus_mails import mails\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "def get_feature_set(mails, fileid, top_words, brown_words):\n",
    "    fd = nltk.FreqDist(token for token in mails.words(fileid) if token.lower() in top_words)\n",
    "    features = {}\n",
    "     \n",
    "    # use one percent as a threshold - could also be tuned\n",
    "    one_percent = math.ceil(fd.N() / 100)\n",
    "\n",
    "    bigrams = nltk.bigrams(mails.words(fileid))\n",
    "\n",
    "    features['many_exclamation_marks'] = fd['!'] > one_percent\n",
    "    features['multiple_exclamation_marks'] = len([(w1.lower(), w2.lower()) for (w1, w2) in bigrams if w1 == \"!\" and w2 == \"!\"]) > 0\n",
    "    features['many_stars'] = fd['*'] > one_percent\n",
    "    features['many_dollars'] = fd['$'] > one_percent\n",
    "    features['multiple_dollars'] = len([(w1.lower(), w2.lower()) for (w1,w2) in bigrams if w1 == \"$\" and w2 == \"$\"]) > 0\n",
    "    features['many_minus'] = fd['-'] > one_percent\n",
    "    features['contains_links'] = len([ word for word in fd.keys() if word. startswith('http')]) > 0\n",
    "    features['many_repetitions'] = len([(w1.lower(), w2.lower()) for (w1,w2) in bigrams if w1 == w2 and w1 in top_words and w2 in top_words]) > one_percent\n",
    "    features['many_out_of_vocab'] = len([w for w in fd.keys() if not w in brown_words]) > one_percent\n",
    "\n",
    "    for token in fd.keys():\n",
    "        features[token] = token\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(mail, train_set, test_set, show_features, class_label, top_words, brown_words):\n",
    "\n",
    "    train_features = [(get_feature_set(mail,fileid,top_words,brown_words), category) for (fileid, category) in\n",
    "    train_set]\n",
    "\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_features)\n",
    "    if show_features:\n",
    "        classifier.show_most_informative_features(20)\n",
    "\n",
    "    results = classifier.classify_many([get_feature_set(mail,fileid,top_words,brown_words) for (fileid, category) in test_set])\n",
    "    gold = [category for (fileid, category) in test_set]\n",
    "\n",
    "    return compute_PRF(gold, results, class_label)\n",
    "\n",
    "def main(email_data, mails):\n",
    "    random.seed(1)\n",
    "    random.shuffle(email_data)\n",
    "    logging.debug(str(email_data[:10]))\n",
    "\n",
    "    # split in training, develop and test set\n",
    "    train_size = int(0.8 * len(email_data))\n",
    "    train_data, test_data = email_data[:train_size], email_data[train_size:]\n",
    "\n",
    "    train_size = int(0.7 * len(train_data))\n",
    "    train_data, develop_data = train_data[:train_size], train_data[train_size:]\n",
    "\n",
    "    logging.debug(\"# training develop test: %s %s %s\", len(train_data), len(develop_data), len(test_data))\n",
    "\n",
    "    # limit word features to 500 top words, as training with many features takes forever\n",
    "    # try different threshold on how many features work well\n",
    "    for threshold in [500]:\n",
    "        logging.info(\"\\nThreshold: %d\" % threshold)\n",
    "\n",
    "        fd_train_words = nltk.FreqDist(w.lower() for w in mails.words(fileids=[f for f, c in train_data]))\n",
    "        top_words = [t for t, _ in fd_train_words.most_common(threshold)]\n",
    "\n",
    "        brown_words = set([w.lower() for w in nltk.corpus.brown.words()])\n",
    "\n",
    "        show_most_informative = False\n",
    "        results = evaluate(mails,train_data, develop_data, show_most_informative, \"spam\", top_words, brown_words)\n",
    "        print(\"precision spam:\", results[0])\n",
    "        print(\"recall spam: \", results[1])\n",
    "        print(\"f-measure spam: \", results[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:[('nospam/8-863msg1.txt', 'nospam'), ('nospam/5-1353msg3.txt', 'nospam'), ('nospam/6-173msg2.txt', 'nospam'), ('nospam/9-1425msg2.txt', 'nospam'), ('nospam/9-148msg1.txt', 'nospam'), ('nospam/6-282msg1.txt', 'nospam'), ('nospam/3-404msg2.txt', 'nospam'), ('nospam/6-200msg1.txt', 'nospam'), ('nospam/6-4msg3.txt', 'nospam'), ('nospam/3-378msg2.txt', 'nospam')]\n",
      "DEBUG:root:# training develop test: 1619 695 579\n",
      "INFO:root:\n",
      "Threshold: 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision spam: 0.8455882352941176\n",
      "recall spam:  0.9745762711864406\n",
      "f-measure spam:  0.905511811023622\n"
     ]
    }
   ],
   "source": [
    "# ORIGINAL SPAM MESSAGES:\n",
    "# prepare review data as a list of tuples (vocab, category)\n",
    "email_data = [(fileid, category) for category in mails_true.categories() for fileid in mails_true.fileids(category)]\n",
    "main(email_data,mails_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nt-2K4MC_UTr"
   },
   "source": [
    "- Report the precision, recall and f-score. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision spam: %s 1.0\n",
      "recall spam: %s 0.9915254237288136\n",
      "f-measure spam: %s 0.9957446808510638\n"
     ]
    }
   ],
   "source": [
    "mails1 = LazyCorpusLoader('mails1', CategorizedPlaintextCorpusReader,r'(?!\\.).*\\.txt', cat_pattern=r'(spam|nospam)/.*')\n",
    "\n",
    "email_data1 = [(fileid, category) for category in mails1.categories() for fileid in mails1.fileids(category)]\n",
    "\n",
    "main(email_data1,mails1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision spam: %s 1.0\n",
      "recall spam: %s 1.0\n",
      "f-measure spam: %s 1.0\n"
     ]
    }
   ],
   "source": [
    "mails2 = LazyCorpusLoader('mails2', CategorizedPlaintextCorpusReader,r'(?!\\.).*\\.txt', cat_pattern=r'(spam|nospam)/.*')\n",
    "\n",
    "email_data2 = [(fileid, category) for category in mails2.categories() for fileid in mails2.fileids(category)]\n",
    "\n",
    "main(email_data2, mails2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lhay5Yf__XsR"
   },
   "source": [
    "- How much can you decrease\n",
    "the classifier’s performance? Discuss results (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfornately the added features does not degrade the classifier performance. The second approache failed because it randomly included word from nltk corpus that are out of brown corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "FolT_Homework_10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}